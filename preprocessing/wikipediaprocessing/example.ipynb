{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, urllib, hashlib\n",
    "\n",
    "path = '/resources/wikipedia/test-extractedResources/'\n",
    "\n",
    "with open(path+'overall_entity_freq.json', 'r') as f:\n",
    "    overall_entity_freq = json.load(f)\n",
    "\n",
    "with open(path+'entities_overall_dict.json', 'r') as f:\n",
    "    entities_overall_dict = json.load(f)\n",
    "\n",
    "with open(path+'entity_inlink_dict.json', 'r') as f:\n",
    "    entity_inlink_dict = json.load(f)\n",
    "\n",
    "with open(path+'entity_outlink_dict.json', 'r') as f:\n",
    "    entity_outlink_dict = json.load(f)\n",
    "\n",
    "with open(path+'mention_overall_dict.json', 'r') as f:\n",
    "    mention_overall_dict = json.load(f)\n",
    "\n",
    "with open(path+'overall_mentions_freq.json', 'r') as f:\n",
    "    overall_mentions_freq = json.load(f)\n",
    "\n",
    "with open(path+'wikipedia2wikidata.json', 'r') as f:\n",
    "    wikipedia2wikidata = json.load(f)\n",
    "\n",
    "with open(path+'wikidata2wikipedia.json', 'r') as f:\n",
    "    wikidata2wikipedia = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Dinghu%20East%20railway%20station', 'freq': 1}]\n"
     ]
    }
   ],
   "source": [
    "## starting point: a wikidata id or a wikipedia title\n",
    "\n",
    "wikidataId = \"Q24833082\"\n",
    "if wikidataId in wikidata2wikipedia:\n",
    "    percent_encoded_title = wikidata2wikipedia[wikidataId][0][\"title\"]\n",
    "    # in case of multiple wikipedia pages pointing to the same id you need to pick the one with the highest freq\n",
    "    print (wikidata2wikipedia[wikidataId])\n",
    "else:\n",
    "    print (\"Missing entity in our wikidata2wikipedia Mapping\")\n",
    "\n",
    "\n",
    "# otherwise is you already know the page title you can do the following\n",
    "\n",
    "#wikipedia_title = \"Dinghu East railway station\"\n",
    "#percent_encoded_title = urllib.parse.quote(wikipedia_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inlinks: {'Zhaoqing%20East%20railway%20station': 1}\n",
      "outlinks: {'Yong%27an%2C%20Zhaoqing': 1, 'Dinghu%20District': 1, 'Zhaoqing': 1, 'Guangdong': 1, 'China': 1, 'Guangzhou%E2%80%93Foshan%E2%80%93Zhaoqing%20intercity%20railway': 1, 'Zhaoqing%20East%20railway%20station': 1}\n",
      "freq: 1\n",
      "name_variations: {'Dinghu East railway station': 1}\n",
      "wikidataId Q24833082\n",
      "Page Content:\n",
      "{'Main': {'order': 1, 'content': ['Dinghu East railway station', '', \"Dinghu East railway station () is a railway station in Yong'an, Dinghu District, Zhaoqing, Guangdong, China. It is an intermediate station on the Guangzhou–Foshan–Zhaoqing intercity railway. It opened with the line on 30 March 2016. The station has two island platforms. It is situated adjacent and perpendicular to Zhaoqing East railway station and a walkway is provided for transfer between the two stations.\"]}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if \"/\" in percent_encoded_title or len(percent_encoded_title)>200:\n",
    "    percent_encoded_title = hashlib.sha224(percent_encoded_title.encode('utf-8')).hexdigest()\n",
    "\n",
    "with open(path+'Pages/'+percent_encoded_title+\".json\", 'r') as f:\n",
    "    page = json.load(f)\n",
    "\n",
    "inlinks = entity_inlink_dict[percent_encoded_title]\n",
    "outlinks = entity_outlink_dict[percent_encoded_title]\n",
    "freq = overall_entity_freq[percent_encoded_title]\n",
    "name_variations = entities_overall_dict[percent_encoded_title]\n",
    "wikidataId = wikipedia2wikidata[percent_encoded_title]\n",
    "\n",
    "print (\"inlinks:\",inlinks)\n",
    "print (\"outlinks:\",outlinks)\n",
    "print (\"freq:\",freq)\n",
    "print (\"name_variations:\",name_variations)\n",
    "print (\"wikidataId\",wikidataId)\n",
    "print (\"Page Content:\")\n",
    "print (page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mention freq: 226\n",
      "entities referred by it: {'London': 222, 'London%20Football%20Association': 1, 'University%20of%20London': 1, 'London%20Fashion%20Week': 2}\n"
     ]
    }
   ],
   "source": [
    "# finally, if you have a mention of an entity and you want to know how many entities can be referred by it\n",
    "\n",
    "mention = \"London\"\n",
    "print (\"mention freq:\", overall_mentions_freq[mention])\n",
    "print (\"entities referred by it:\", mention_overall_dict[mention])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c81a5a1980dacfbd83d833a7e56ee30243d7c1ccda80563348485af74244aa8e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('py39resolution': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
